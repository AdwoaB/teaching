{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\"> \n",
    "DATS 6103 - 10, Summer 2018, Homework_1_solution\n",
    "</h1> \n",
    "\n",
    "<h1 align=\"center\"> \n",
    "Due May 29, 11:59 PM\n",
    "</h1> \n",
    "\n",
    "<h4 align=\"center\"> \n",
    "Author: Yuxiao Huang ([yuxiaohuang@gwu.edu](mailto:yuxiaohuang@gwu.edu))\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Note\n",
    "- Complete the missing parts indicated by **# Implement me**\n",
    "- Submit an ipynb file named **Homework_1.ipynb** to [blackboard](https://blackboard.gwu.edu) folder /Assignments/Homework_1/\n",
    "-  We expect you to follow a reasonable programming style. While we do not mandate a specific style, we require that your code to be neat, clear, **documented/commented** and above all consistent. **Marks will be deducted if these are not followed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is crucial in areas such as Data Mining and Machine Learning. Data preprocessing is comprised of a sequence of steps such as splitting the data into training and testing, and standardization. For now, let us focus on the implementation of the two steps. We will come back to this in greater detail later. First let us load [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal length</th>\n",
       "      <th>Sepal width</th>\n",
       "      <th>Petal length</th>\n",
       "      <th>Petal width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal length  Sepal width  Petal length  Petal width       target\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "                 header=None, \n",
    "                 names=['Sepal length', 'Sepal width', 'Petal length', 'Petal width', 'target'])\n",
    "\n",
    "# Show the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we divide the original dataset into two non-overlapping subsets. One set is called the training set, and the other testing set. We use a predetermined value, $train\\_size$, for determining the proportion of the original dataset to include in the training set. For example, $train\\_size=0.7$ means 70% of the data in the original dataset will be put into the training set, whereas the remaining 30% will be put into the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_test_split(df, train_size=0.7):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : data (dataframe) \n",
    "    train_size : the proportion of the original dataset to include in the training set\n",
    "                 0.7 by default\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_train : training set (dataframe)\n",
    "    df_test : testing set (dataframe)\n",
    "    \"\"\"\n",
    "\n",
    "    # Specify the seed of the pseudo random number generator\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Get the number of rows in the original dataset\n",
    "    row_num = df.shape[0]\n",
    "\n",
    "    # Get the number of rows in the training set\n",
    "    row_num_train = int(train_size * row_num)\n",
    "\n",
    "    # Randomly choose row_num_train rows from the original dataset\n",
    "    # These rows will be included into the training set\n",
    "    row_idxs_train = np.random.choice(row_num, size=row_num_train, replace=False)\n",
    "\n",
    "    # Get the remaining rows from the original dataset\n",
    "    # These rows will be included into the testing set\n",
    "    row_idxs_test = [i for i in range(row_num) if i not in row_idxs_train]\n",
    "\n",
    "    # Get the training set\n",
    "    df_train = df.iloc[row_idxs_train, :]\n",
    "\n",
    "    # Get the testing set\n",
    "    df_test = df.iloc[row_idxs_test, :]\n",
    "    \n",
    "    return [df_train, df_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-08620356e859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split the data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Show the first five rows of the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The first five rows in the training set:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cae032e46f3f>\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(df, train_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Randomly choose row_num_train rows from the original dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# These rows will be included into the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mrow_idxs_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_num_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Get the remaining rows from the original dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "df_train, df_test = train_test_split(df)\n",
    "\n",
    "# Show the first five rows of the training set\n",
    "print('The first five rows in the training set:')\n",
    "print(df_train.head())\n",
    "print()\n",
    "\n",
    "# Show the first five rows of the testing set\n",
    "print('The first five rows in the testing set:')\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features\n",
    "features = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "\n",
    "# Get the feature vector in the training set\n",
    "X_train = df_train[features]\n",
    "\n",
    "print('The first five rows in X_train:')\n",
    "print(X_train.head())\n",
    "print()\n",
    "\n",
    "# Get the feature vector in the testing set\n",
    "X_test = df_test[features]\n",
    "\n",
    "print('The first five rows in X_test:')\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing the training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we standardize the training and testing sets. Specifically, standardizing the features in the training set works as follows. For each feature $\\mathrm{x}$, we first calculate the $\\mu$ (mean) and $\\sigma$ (standard deviation). This process is usually called \"fit\". We then calculate the $z$-value:\n",
    "\n",
    "$$\n",
    "\\mathrm{z} = \\frac{\\mathrm{x} - \\mu}{\\sigma}.\n",
    "$$\n",
    "\n",
    "Here, $\\mathrm{z}$ is the standardization of feature $\\mathrm{x}$, usually denoted by $\\mathrm{X}_{std}$. This process is usually called \"transform\".\n",
    "\n",
    "When standardizing the features in the testing set, however, we directly use the $\\mu$ and $\\sigma$ of each feature calculated in the training set to calculate the $\\mathrm{X}_{std}$ (using the above equation). In other words, here we do not \"fit\" but only \"transform\" the data. Again, here we only focus on the implementation. The reason behind the difference will be discussed in detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fit process in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calcualte the mean of each feature in the training set\n",
    "means = np.mean(X_train)\n",
    "\n",
    "# Calculate the std of each feature in the training set\n",
    "stds = np.std(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The transform process in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the standardization of each feature in the training set\n",
    "X_train_std = (X_train - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The transform process in the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the standardization of each feature in the testing set\n",
    "X_test_std = (X_test - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) in [scikit-learn](http://scikit-learn.org/stable/index.html)\n",
    "Here we compare the performance of our standard scaler with the StandardScaler in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Declare the StandardScaler object\n",
    "StdScaler = StandardScaler()\n",
    "\n",
    "# Standardize X_train using the .fit_transform() function\n",
    "X_train_std_sklearn = StdScaler.fit_transform(X_train)\n",
    "\n",
    "# Standardize X_test using the .transform() function\n",
    "X_test_std_sklearn = StdScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calcuate and display the Mean Square Error (MSE)\n",
    "# based on the standardization produced by StandardScaler and that produced by your implementation\n",
    "# Since the seed of the pseudo random number generator was specified,\n",
    "# you should see the exact result when your implementation is correct\n",
    "print('MSE on the training set: ' + str(mean_squared_error(X_train_std_sklearn, X_train_std)))\n",
    "print('MSE on the testing set: ' + str(mean_squared_error(X_test_std_sklearn, X_test_std)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
